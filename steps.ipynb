{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by step\n",
    "This notebook replicates the code on the 'functions' page, offering insight into the inner workings of the functions. This allows anyone interested in comprehending and modifying the code to gain a general understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shap\n",
    "import re\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "import json\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import OrdinalEncoder, RobustScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "The preprocessing step aims to get the data ready for the model to learn from. This includes making sure the data is in the right format and cleaning it up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports excell with patients data\n",
    "data_path = \"./testData/dummy_data.xlsx\"\n",
    "\n",
    "# Read the uploaded Excel file into a Pandas DataFrame\n",
    "xls = pd.ExcelFile(data_path, engine=\"openpyxl\")\n",
    "\n",
    "sheet_names = ['Baseline', 'TEG Values', 'Events']  # Replace with your sheet names\n",
    "\n",
    "# Access each sheet's data using the sheet name as the key\n",
    "baseline_df = pd.read_excel(xls, sheet_names[0])\n",
    "tegValues_df = pd.read_excel(xls, sheet_names[1])\n",
    "events_df = pd.read_excel(xls, sheet_names[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tegValues_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge tables\n",
    "The data is currently split into three tables. To make it usable for the model, we need to combine all the important information into a two table, one with the baseline information and the other one with the TEG values\n",
    "\n",
    "All the events for every patient will be counted and added to a column called \"Events\" (Count encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to image\n",
    "image_path = \"./data/data_structure.png\"\n",
    "# Display the image\n",
    "display(Image(filename=image_path, width=300, height=200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of events for each 'Record_ID' in events_df\n",
    "event_counts = events_df['Record ID'].value_counts().reset_index()\n",
    "event_counts.columns = ['Record ID', 'Events']\n",
    "event_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the event counts with the baseline and teg values\n",
    "tegValues_df = tegValues_df.merge(event_counts, on='Record ID', how='left')\n",
    "baseline_df = baseline_df.merge(event_counts, on='Record ID', how='left')\n",
    "\n",
    "# Fill NaN values in the 'event_count' column with 0\n",
    "tegValues_df['Events'].fillna(0, inplace=True)\n",
    "baseline_df['Events'].fillna(0, inplace=True)\n",
    "tegValues_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in excel\n",
    "excel_file = \"./testData/merged_data.xlsx\"\n",
    "\n",
    "# Create an Excel writer object\n",
    "with pd.ExcelWriter(excel_file, engine='xlsxwriter') as writer:\n",
    "    # Write each DataFrame to a different Excel sheet\n",
    "    tegValues_df.to_excel(writer, sheet_name='TEG values', index=False)\n",
    "    baseline_df.to_excel(writer, sheet_name='Baseline', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformations\n",
    "All columns are being transformed to the best fitting format, according to the information they hold and effectively removing any typos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean df in new copy\n",
    "clean_TEG_df = tegValues_df.copy()\n",
    "clean_baseline_df = baseline_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number\n",
    "Baseline:\n",
    "- Age\n",
    "- BMI\n",
    "- Clotting Disorder\n",
    "- EGFR (mL/min/1.73m2)\n",
    "- BP prior to blood draw\n",
    "- ABI Right\n",
    "- ABI Left\n",
    "- Rutherford Score\n",
    "\n",
    "TEG:\n",
    "- TEG values\n",
    "- Visit Timepoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find teg values column\n",
    "columns_to_exclude = ['Record ID', 'Visit Timepoint', 'Antiplatelet Therapy within 7 Days',\n",
    "                      'Anticoagulation within 24 Hours', 'Statin within 24 Hours', 'Cilostazol within 7 days',\n",
    "                      'BP prior to blood draw', 'Events']\n",
    "\n",
    "tegValues = [col for col in tegValues_df.columns.values if col not in columns_to_exclude]\n",
    "tegValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_columns_baseline = [\"Age\",\"BMI\", \"Clotting Disorder\", \"EGFR (mL/min/1.73m2)\", \"ABI Right\", \"ABI left\", \"Rutherford Score\"]\n",
    "number_columns_teg = [\"Visit Timepoint\", \"BP prior to blood draw\"]+tegValues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the values and their types to identify the kind of changes needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_TEG_df[number_columns_teg].dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_baseline_df[number_columns_baseline].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_TEG_df[number_columns_teg].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_baseline_df[number_columns_baseline].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the columns visuzlied, age, BMI and clotting dissorder are in the right format.\n",
    "\n",
    "BP needs to be split between systolic and diastolic and made into ints.\n",
    "\n",
    "EGFR is a combination of strings and floats. The string is \">60\", which can be approximated to a big number, like 65. All the other values are floats.\n",
    "TEG values need to be transformed to floats. Some teg values have maximum value stored as \">n\", or say \"inconclusive\" or other string when data was not colected. Those vaues wil be marked as nan\n",
    "Both TEG values and EGFR boundary conditions are saved in the \"./data_boundary.json\" file\n",
    "\n",
    "Visit timepoint is in strings and need to be based on days\n",
    "\n",
    "ABI left and right have some strings that will be converted to NaN values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split BP into two columns (systolic and diastolic) based on \"/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the column into 'Systolic' and 'Diastolic' columns\n",
    "clean_TEG_df[['BP_Systolic', 'BP_Diastolic']] = clean_TEG_df['BP prior to blood draw'].str.split('/', expand=True)\n",
    "\n",
    "# Convert 'Systolic' and 'Diastolic' columns to integers\n",
    "clean_TEG_df['BP_Systolic'] = pd.to_numeric(clean_TEG_df['BP_Systolic'], errors='coerce').astype('Int64')\n",
    "clean_TEG_df['BP_Diastolic'] = pd.to_numeric(clean_TEG_df['BP_Diastolic'], errors='coerce').astype('Int64')\n",
    "\n",
    "# Drop the first column 'BP prior to blood draw'\n",
    "clean_TEG_df.drop(columns=['BP prior to blood draw'], inplace = True)\n",
    "number_columns_teg.remove('BP prior to blood draw')\n",
    "number_columns_teg.append('BP_Systolic')\n",
    "number_columns_teg.append('BP_Diastolic')\n",
    "\n",
    "clean_TEG_df[['BP_Systolic', 'BP_Diastolic']].dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean EGFR and TEG data with boundary values and convert all to floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import boundary values\n",
    "\n",
    "# Get the current working directory (base directory)\n",
    "base_directory = os.getcwd()\n",
    "\n",
    "# Define the filename\n",
    "filename = 'data_boundaries.json'\n",
    "\n",
    "# Create the full file path by joining the base directory and filename\n",
    "file_path = os.path.join(base_directory, 'data', filename)\n",
    "\n",
    "with open(file_path, 'r') as json_file:\n",
    "    boundaries = json.load(json_file)\n",
    "boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all boundary values with their correcponding right values\n",
    "\n",
    "# EGFR\n",
    "egfr_column = 'EGFR (mL/min/1.73m2)'\n",
    "efgr_replacement = boundaries.pop(egfr_column, None)\n",
    "# Remove spaces in the column\n",
    "clean_baseline_df[egfr_column] = clean_baseline_df[egfr_column].replace(regex={r'\\s': ''})\n",
    "\n",
    "# Use a regular expression to match and replace values\n",
    "for name, replacement in efgr_replacement.items():\n",
    "    clean_baseline_df[egfr_column] = clean_baseline_df[egfr_column].replace({f'^{name}': replacement}, regex=True)\n",
    "\n",
    "# Iterate over TEG DataFrame and apply boundaries\n",
    "for column, replacement_dict in boundaries.items():\n",
    "    \n",
    "    # Remove spaces in the column\n",
    "    clean_TEG_df[column] = clean_TEG_df[column].replace(regex={r'\\s': ''})\n",
    "    \n",
    "    # Use a regular expression to match and replace values\n",
    "    for name, replacement in replacement_dict.items():\n",
    "        clean_TEG_df[column] = clean_TEG_df[column].replace({f'^{name}': replacement}, regex=True)\n",
    "\n",
    "# Show changes    \n",
    "clean_TEG_df[list(boundaries.keys())].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show changes\n",
    "clean_baseline_df[egfr_column].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert  Rutherford Score and TEG values to float\n",
    "clean_baseline_df[\"Rutherford Score\"] = pd.to_numeric(clean_baseline_df[\"Rutherford Score\"], errors='coerce')\n",
    "clean_baseline_df[\"Rutherford Score\"].dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the columns and convert to numeric\n",
    "for column in tegValues:\n",
    "    clean_TEG_df[column] = pd.to_numeric(clean_TEG_df[column], errors='coerce')\n",
    "\n",
    "clean_TEG_df[tegValues].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show values to make sure strings were changed to NaN\n",
    "clean_TEG_df[tegValues].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change timepoints from strings to ints that represent days after the operation.\n",
    "\n",
    "All the values are saved in ./data/timepoints.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filename\n",
    "filename = 'timepoints.json'\n",
    "\n",
    "# Create the full file path by joining the base directory and filename\n",
    "file_path = os.path.join(base_directory, 'data', filename)\n",
    "\n",
    "\n",
    "with open(file_path, 'r') as json_file:\n",
    "    timepoints = json.load(json_file)\n",
    "timepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reverse mapping dictionary\n",
    "reverse_mapping = {v: k for k, values in timepoints.items() for v in values}\n",
    "\n",
    "# Replace values using the reverse mapping\n",
    "clean_TEG_df['Days from operation'] = clean_TEG_df['Visit Timepoint'].map(reverse_mapping)\n",
    "\n",
    "# Convert the column to integer\n",
    "clean_TEG_df['Days from operation'] = clean_TEG_df['Days from operation'].astype(int)\n",
    "\n",
    "# Drop old column\n",
    "clean_TEG_df.drop(columns=['Visit Timepoint'], inplace = True)\n",
    "number_columns_teg.remove('Visit Timepoint')\n",
    "number_columns_teg.append('Days from operation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_TEG_df['Days from operation'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert ABI values to floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_baseline_df['ABI Right'] = pd.to_numeric(clean_baseline_df['ABI Right'], errors='coerce')\n",
    "clean_baseline_df['ABI left'] = pd.to_numeric(clean_baseline_df['ABI left'], errors='coerce')\n",
    "\n",
    "clean_baseline_df[['ABI Right', 'ABI left']].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appreciate all your work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_baseline_df[number_columns_baseline].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_TEG_df[number_columns_teg].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booleans\n",
    "Baseline:\n",
    "- Sex\n",
    "- White\n",
    "- Diabetes\n",
    "- Hypertension\n",
    "- Hyperlipidemia\n",
    "- Coronary Artery Disease\n",
    "- History of MI \n",
    "- Functional impairment\n",
    "- Does Subject Currently have cancer?\n",
    "- Past hx of cancer\n",
    "- Hx of  DVT\n",
    "- Hx of stroke\n",
    "- Hx of pulmonary embolism:\n",
    "- Does the patient have a history of solid organ transplant?\n",
    "- Has subject had previous intervention of the index limb? \n",
    "- Previous occluded stents\n",
    "\n",
    "TEG values:\n",
    "- Cilostazol within 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'Is Male' column based on the 'sex' column\n",
    "clean_baseline_df['Is Male'] = (clean_baseline_df['Sex'] == 'Male').astype(bool)\n",
    "\n",
    "# Drop the old 'sex' column\n",
    "clean_baseline_df.drop('Sex', axis=1, inplace=True)\n",
    "clean_baseline_df['Is Male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change following columns to booleans\n",
    "columns_to_convert_baseline = ['White', 'Diabetes', 'Hypertension', 'Hyperlipidemia (choice=None)', 'Coronary Artery Disease', 'History of MI',\n",
    "                      'Functional impairment', 'Does Subject Currently have cancer?', 'Past hx of cancer', 'Hx of  DVT', 'Hx of stroke',\n",
    "                      'Hx of pulmonary embolism', 'Does the patient have a history of solid organ transplant?', \n",
    "                      'Has subject had previous intervention of the index limb?', 'Previous occluded stents',]\n",
    "columns_to_convert_TEG =['Cilostazol within 7 days']\n",
    "\n",
    "clean_baseline_df[columns_to_convert_baseline].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_TEG_df[columns_to_convert_TEG].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for replacement\n",
    "replacement_dict = {'yes': True, 'no': False, '1': True, '0': False, 'cilostazol': True, 'NaN':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with False\n",
    "clean_baseline_df[columns_to_convert_baseline] = clean_baseline_df[columns_to_convert_baseline].fillna('0')\n",
    "clean_TEG_df[columns_to_convert_TEG] = clean_TEG_df[columns_to_convert_TEG].fillna('0')\n",
    "\n",
    "# Put all columns in lowercase\n",
    "clean_baseline_df[columns_to_convert_baseline] = clean_baseline_df[columns_to_convert_baseline].astype(str)\n",
    "clean_baseline_df[columns_to_convert_baseline] = clean_baseline_df[columns_to_convert_baseline].apply(lambda x: x.str.lower())\n",
    "clean_TEG_df[columns_to_convert_TEG] = clean_TEG_df[columns_to_convert_TEG].astype(str)\n",
    "clean_TEG_df[columns_to_convert_TEG] = clean_TEG_df[columns_to_convert_TEG].apply(lambda x: x.str.lower())\n",
    "\n",
    "# Use the replace method to replace values in multiple columns\n",
    "clean_baseline_df[columns_to_convert_baseline] = clean_baseline_df[columns_to_convert_baseline].replace(replacement_dict).astype(bool)\n",
    "clean_TEG_df[columns_to_convert_TEG] = clean_TEG_df[columns_to_convert_TEG].replace(replacement_dict).astype(bool)\n",
    "\n",
    "clean_baseline_df[columns_to_convert_baseline].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_TEG_df[columns_to_convert_TEG].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical ordinal\n",
    "Baseline:\n",
    "- Tobacco Use\n",
    "- Renal Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal encoding map\n",
    "category_orders = {\n",
    "    'Tobacco Use (1 current 2 former, 3 none)': \n",
    "    ['None',\n",
    "    'Past, quit >10 year ago',\n",
    "    'quit 1 to 10 years ago', \n",
    "    'current within the last year ( < 1 pack a day)',\n",
    "    'current within the last year (  > or = 1 pack a day)'],\n",
    "\n",
    "    'Renal Status': \n",
    "    ['Normal', \n",
    "    'GFR 30 to 59', \n",
    "    'GFR 15 to 29', \n",
    "    'GFR<15 or patient is on dialysis',\n",
    "    '1']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace renal status values. Some of the values in the data set mean the same with different words\n",
    "# Define a dictionary to map old values to new values\n",
    "replace_dict = {'GFR 60 to 89': 'Normal', 'Evidence of renal dysfunction ( GFR >90)': 'Normal', '0': 'Normal', 0: 'Normal', 1: \"1\"}\n",
    "\n",
    "clean_baseline_df['Renal Status'] = clean_baseline_df['Renal Status'].replace(replace_dict)\n",
    "\n",
    "# Initialize the OrdinalEncoder with specified category orders\n",
    "encoder = OrdinalEncoder(categories=[category_orders[column] for column in ['Tobacco Use (1 current 2 former, 3 none)', 'Renal Status']])\n",
    "\n",
    "# Fit and transform the selected columns to encode ordinal values\n",
    "clean_baseline_df[['Tobacco Use (1 current 2 former, 3 none)', 'Renal Status']] = encoder.fit_transform(clean_baseline_df[['Tobacco Use (1 current 2 former, 3 none)', 'Renal Status']])\n",
    "\n",
    "# Rename column\n",
    "clean_baseline_df = clean_baseline_df.rename(columns={'Tobacco Use (1 current 2 former, 3 none)': 'Tobacco Use'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_baseline_df[['Tobacco Use', 'Renal Status']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical nominal\n",
    "Baseline:\n",
    "- Extremity\n",
    "- Artery affected\n",
    "- Intervention Classification\n",
    "- Intervention Type\n",
    "\n",
    "TEG values:\n",
    "- Antiplatelet Therapy within 7 Days\n",
    "- Anticoagulation within 24 Hours\n",
    "- Statin within 24 Hours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_dummy_baseline = ['Extremity',\n",
    "                    'Intervention Classification']\n",
    "columns_to_dummy_TEG = ['Statin within 24 Hours']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy encoding of categorical values\n",
    "clean_baseline_df = pd.get_dummies(clean_baseline_df, columns=columns_to_dummy_baseline,\n",
    "                    prefix=columns_to_dummy_baseline)\n",
    "clean_TEG_df = pd.get_dummies(clean_TEG_df, columns=columns_to_dummy_TEG,\n",
    "                    prefix=columns_to_dummy_TEG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unecessary columns\n",
    "clean_baseline_df = clean_baseline_df.drop(columns=['Extremity_left']) # Because it is either right, left or bilateral\n",
    "clean_baseline_df = clean_baseline_df.drop(columns=['Intervention Classification_Endo']) # Either endo, open or combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show columns \n",
    "# Use the .filter() method to select columns with the original columns prefixes\n",
    "dummy_columns_baseline = [col for col in clean_baseline_df.columns if any(col.startswith(prefix) for prefix in columns_to_dummy_baseline)]\n",
    "clean_baseline_df[dummy_columns_baseline].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_columns_TEG = [col for col in clean_TEG_df.columns if any(col.startswith(prefix) for prefix in columns_to_dummy_TEG)]\n",
    "clean_TEG_df[dummy_columns_TEG].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _Artery affected_, _Intervention type_, _Antiplatelet Therapy within 7 Days_, and _Anticoagulation within 24 Hours_ column has multiple values in a sigle string. They will be normalized before being encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artery affected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique valuses\n",
    "unique_arteries = set()\n",
    "unique_antiplatelet = set()\n",
    "unique_intervention = set()\n",
    "unique_anticoagulation = set()\n",
    "\n",
    "for index, row in clean_baseline_df.iterrows():\n",
    "    arteries = row['Artery affected'].split(', ')\n",
    "    unique_arteries.update(arteries)\n",
    "\n",
    "    intervention = row['Intervention Type'].split(', ')\n",
    "    unique_intervention.update(intervention)\n",
    "    \n",
    "\n",
    "for index, row in clean_TEG_df.iterrows():\n",
    "\n",
    "    antiplatelet = row['Antiplatelet Therapy within 7 Days'].split(', ')\n",
    "    unique_antiplatelet.update(antiplatelet)\n",
    "\n",
    "    anticoagulation = row['Anticoagulation within 24 Hours'].split(', ')\n",
    "    # Delete items in parenthesis ex: heparin (Calciparine) to be just heparin\n",
    "    anticoagulation = {re.sub(r'\\s*\\([^)]*\\)\\s*', '', item) for item in anticoagulation} \n",
    "    unique_anticoagulation.update(anticoagulation)\n",
    "\n",
    "\n",
    "print(unique_arteries)\n",
    "print(unique_antiplatelet)\n",
    "print(unique_intervention)\n",
    "print(unique_anticoagulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy encode ateries affected\n",
    "selected_arteries = []\n",
    "for artery in unique_arteries:\n",
    "    column_name = \"Artery affected_\"+artery\n",
    "    clean_baseline_df[column_name] = clean_baseline_df['Artery affected'].str.contains(artery).astype(int)\n",
    "    selected_arteries.append(column_name)\n",
    "\n",
    "selected_arteries.append('Artery affected')\n",
    "clean_baseline_df[selected_arteries].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy encode antiplatelete therapy\n",
    "selected_antiplatelet = []\n",
    "for antiplatelet in unique_antiplatelet:\n",
    "    column_name = \"Antiplatelet therapy_\"+antiplatelet\n",
    "    clean_TEG_df[column_name] = clean_TEG_df['Antiplatelet Therapy within 7 Days'].str.contains(antiplatelet).astype(int)\n",
    "    selected_antiplatelet.append(column_name)\n",
    "\n",
    "selected_antiplatelet.append('Antiplatelet Therapy within 7 Days')\n",
    "clean_TEG_df[selected_antiplatelet].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy encode intervention types\n",
    "selected_intervention = []\n",
    "for intervention in unique_intervention:\n",
    "    column_name = 'Intervention type_'+intervention\n",
    "    clean_baseline_df[column_name] = clean_baseline_df['Intervention Type'].str.contains(intervention).astype(int)\n",
    "    selected_intervention.append(column_name)\n",
    "\n",
    "selected_intervention.append('Intervention Type')\n",
    "clean_baseline_df[selected_intervention].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy encode anticoagulation meds\n",
    "selected_anticoagulation = []\n",
    "for anticoagulation in unique_anticoagulation:\n",
    "    column_name = \"Anticoagulation_\"+anticoagulation\n",
    "    clean_TEG_df[column_name] = clean_TEG_df['Anticoagulation within 24 Hours'].str.contains(anticoagulation).astype(int)\n",
    "    selected_anticoagulation.append(column_name)\n",
    "\n",
    "selected_anticoagulation.append('Anticoagulation within 24 Hours')\n",
    "clean_TEG_df[selected_anticoagulation].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop old columns\n",
    "clean_baseline_df.drop(columns=['Artery affected','Intervention Type'], inplace=True)\n",
    "clean_TEG_df.drop(columns=['Antiplatelet Therapy within 7 Days', 'Anticoagulation within 24 Hours'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in excel\n",
    "excel_file = \"./testData/clean_data.xlsx\"\n",
    "\n",
    "# Create an Excel writer object\n",
    "with pd.ExcelWriter(excel_file, engine='xlsxwriter') as writer:\n",
    "    # Write each DataFrame to a different Excel sheet\n",
    "    clean_TEG_df.to_excel(writer, sheet_name='TEG values', index=False)\n",
    "    clean_baseline_df.to_excel(writer, sheet_name='Baseline', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend data\n",
    "Create the rate of change of teg values colum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User selects to extend data\n",
    "user_extend_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns\n",
    "tegValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_df = clean_TEG_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if user_extend_data:    \n",
    "    # Sort the DataFrame by \"Record ID\" and \"Visit Timepoint\"\n",
    "    extended_df= extended_df.sort_values(by=[\"Record ID\", \"Days from operation\"])\n",
    "    extended_df[[\"Record ID\", \"Days from operation\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if user_extend_data:\n",
    "    # Group by 'Record ID'\n",
    "    grouped = extended_df.groupby('Record ID')\n",
    "\n",
    "    #Calculate the difference in 'Days from operation'\n",
    "    extended_df['Days Diff'] = grouped['Days from operation'].diff()\n",
    "\n",
    "    # Replace 0s to avoid infinity\n",
    "    extended_df[\"Days Diff\"] = extended_df[\"Days Diff\"].replace(0, 1)\n",
    "\n",
    "    extended_df[[\"Record ID\", \"Days from operation\", \"Days Diff\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if user_extend_data:\n",
    "    new_columns = []\n",
    "    # Iterate TEG values\n",
    "    for value in tegValues:\n",
    "\n",
    "        # Get column names\n",
    "        diff_column_name = f\"{value}_difference_since_last_timepoint\"\n",
    "        rate_column_name = f\"{value}_rate_since_last_timepoint\"\n",
    "        new_columns.append(diff_column_name)\n",
    "        new_columns.append(rate_column_name)\n",
    "\n",
    "\n",
    "        # Calculate the difference in TEG values\n",
    "        extended_df[diff_column_name] = grouped[value].diff()\n",
    "\n",
    "        # Divide  by the differences in 'Days from operation'\n",
    "        extended_df[rate_column_name] = extended_df[diff_column_name] / extended_df['Days Diff']\n",
    "\n",
    "    # Fill the first value with the next one to avoid NaN\n",
    "    extended_df.bfill(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if user_extend_data:\n",
    "    extended_df[new_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if user_extend_data:\n",
    "    # Drop column with diff in dates\n",
    "    extended_df.drop(columns=[\"Days Diff\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if user_extend_data:\n",
    "    # Save in excel\n",
    "    excel_file = \"./testData/extended_data.xlsx\"\n",
    "\n",
    "    # Create an Excel writer object\n",
    "    with pd.ExcelWriter(excel_file, engine='xlsxwriter') as writer:\n",
    "        # Write each DataFrame to a different Excel sheet\n",
    "        extended_df.to_excel(writer, sheet_name='TEG values', index=False)\n",
    "        clean_baseline_df.to_excel(writer, sheet_name='Baseline', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization\n",
    "The goal of this section is to create the graphs that will be shown to the user describing the general data demographics\n",
    "Some of the values are calculated based on the totaal number of patients in the baseline information, and some is calculated from the TEG values\n",
    "\n",
    "Baseline summary:\n",
    "- Age\n",
    "- Gender\n",
    "- Ethnicity\n",
    "- BMI\n",
    "\n",
    "TEG values:\n",
    "- Number of events\n",
    "- Total number of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_df = clean_baseline_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom colors\n",
    "male_colors = ['#d9ed92', '#99d98c'] \n",
    "white_colors = ['#184e77', '#1a759f'] \n",
    "events_colors = '#1a759f'\n",
    "age_histogram_color = '#52b69a' \n",
    "bmi_histogram_color = '#1e6091'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count binary values in the \"Male\" column\n",
    "male_counts = fig_df['Is Male'].value_counts()\n",
    "male_labels = ['Male' if male_counts.index[0] else 'Female', 'Male' if not male_counts.index[0] else 'Female']\n",
    "# Create a pie chart for \"Male\" with custom colors\n",
    "sex_pie = go.Pie(labels=male_labels, values=male_counts, marker=dict(colors=male_colors))\n",
    "\n",
    "# Visualize\n",
    "data = [sex_pie]\n",
    "fig = go.Figure(data = data)\n",
    "fig.update_layout(width=300, height=300)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count binary values in the \"White\" column\n",
    "white_counts = fig_df['White'].value_counts()\n",
    "white_labels = ['White' if white_counts.index[0] else 'Non-White', 'White' if not white_counts.index[0] else 'Non-White']\n",
    "\n",
    "# Create a pie chart for \"White\" with custom colors\n",
    "white_pie = go.Pie(labels=white_labels, values=white_counts, marker=dict(colors=white_colors))\n",
    "\n",
    "# Visualize\n",
    "data = [white_pie]\n",
    "fig = go.Figure(data = data)\n",
    "fig.update_layout(width=300, height=300)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BMI histogram\n",
    "bmi_hist =  go.Histogram(x=fig_df[\"BMI\"], name=\"BMI\", marker=dict(color=bmi_histogram_color))\n",
    "\n",
    "# Visualize\n",
    "data = [bmi_hist]\n",
    "fig = go.Figure(data = data)\n",
    "fig.update_layout(width=300, height=300)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age histogram\n",
    "age_hist=  go.Histogram(x=fig_df[\"Age\"], name=\"Age\", marker=dict(color=age_histogram_color))\n",
    "\n",
    "# Visualize\n",
    "data = [age_hist]\n",
    "fig = go.Figure(data = data)\n",
    "fig.update_layout(width=300, height=300)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following metrics are bsed on the total number of TEG test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy TEG df to find metrics\n",
    "fig_df = clean_TEG_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Events histogram \n",
    "events_hist =  go.Histogram(x=fig_df[\"Events\"], name=\"Events\", marker=dict(color=events_colors))\n",
    "\n",
    "# Visualize\n",
    "data = [events_hist]\n",
    "fig = go.Figure(data = data)\n",
    "fig.update_layout(width=300, height=300)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table\n",
    "unique_patients = fig_df['Record ID'].nunique()\n",
    "total_data_points = len(fig_df)\n",
    "\n",
    "data_summary = pd.DataFrame({\n",
    "    'Category': ['Unique Patients', 'Total Data Points'],\n",
    "    'Count': [unique_patients, total_data_points]\n",
    "})\n",
    "\n",
    "patients_table = go.Table(\n",
    "    header=dict(values=[\"Category\", \"Count\"]),\n",
    "    cells=dict(values=[data_summary['Category'], data_summary['Count']])\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "data = [patients_table]\n",
    "fig = go.Figure(data = data)\n",
    "fig.update_layout(width=300, height=300)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig = make_subplots(rows=2, cols=3,\n",
    "                    specs=[[{'type':'domain'}, {'type':'domain'},{'type':'xy'}],\n",
    "                            [{'type':'xy'}, {'type':'xy'},{'type':'domain'}]],\n",
    "                    subplot_titles=['Gender Distribution', 'Ethnicity Distribution', 'Thrombotic event', 'BMI',\n",
    "                                    'Age', 'Data Summary'])\n",
    "\n",
    "fig.add_trace(sex_pie, row=1, col=1)\n",
    "fig.add_trace(white_pie, row=1, col=2)\n",
    "fig.add_trace(events_hist, row=1, col=3)\n",
    "fig.add_trace(bmi_hist, row=2, col=1)\n",
    "fig.add_trace(age_hist, row=2, col=2)\n",
    "fig.add_trace(patients_table, row=2, col=3)\n",
    "\n",
    "fig.update_layout(width=900, height=600)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model function\n",
    "There will be three models trained, so a function is being created now to be used multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, target_column, drop_columns):\n",
    "    \"\"\"\n",
    "    Trains an XGBoost regression model on the given DataFrame using grid search for hyperparameter tuning.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame containing the features and target variable.\n",
    "    - target_column (str): The name of the target variable column.\n",
    "    - drop_columns (list): List of column names to be dropped from the feature set.\n",
    "\n",
    "    Returns:\n",
    "    - best_pipeline (Pipeline): The best-performing pipeline after hyperparameter tuning.\n",
    "\n",
    "    Example:\n",
    "    best_model = train_model(df=my_dataframe, target_column='target', drop_columns=['column1', 'column2'])\n",
    "    \"\"\"\n",
    "\n",
    "    # Separate features (X) and target (y)\n",
    "    y = df[target_column]\n",
    "    X = df.drop(labels=drop_columns + [target_column], axis=1)\n",
    "\n",
    "    # Split data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create transformers for feature scaling\n",
    "    feature_scaler = RobustScaler()\n",
    "    target_scaler = MinMaxScaler()\n",
    "\n",
    "    # Create a pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('feature_scaler', feature_scaler),  # Robust scaling for features\n",
    "        ('target_scaler', target_scaler),    # Min-Max scaling for the target\n",
    "        ('xgb_regressor', XGBRegressor())    # XGBoost regressor\n",
    "    ])\n",
    "\n",
    "    # Define hyperparameter grid for tuning (adjust as needed)\n",
    "    param_grid = {\n",
    "        'xgb_regressor__max_depth': [3, 4, 5],\n",
    "        'xgb_regressor__gamma': [0, 0.1, 0.2],\n",
    "        'xgb_regressor__min_child_weight': [1, 2, 5]\n",
    "    }\n",
    "\n",
    "    # Initialize K-Fold cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize GridSearchCV for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid,\n",
    "                               scoring='r2', cv=kf)\n",
    "\n",
    "    # Fit the model and perform hyperparameter tuning\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "    # Access the best pipeline\n",
    "    best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = best_pipeline.predict(X_test)  \n",
    "    # Evaluate the model using Mean Squared Error\n",
    "    mse_test = mean_squared_error(y_test, y_pred)\n",
    "    # Calculate R-squared (R2) score\n",
    "    r2_test = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Make predictions on the train data\n",
    "    y_pred = best_pipeline.predict(X_train)  \n",
    "    # Evaluate the model using Mean Squared Error\n",
    "    mse_train = mean_squared_error(y_train, y_pred)\n",
    "    # Calculate R-squared (R2) score\n",
    "    r2_train = r2_score(y_train, y_pred)\n",
    "    \n",
    "    score = {\"mse test\":mse_test, \"r2 test\": r2_test, \"mse train\": mse_train, \"r2 train\": r2_train}\n",
    "\n",
    "    return best_pipeline, X_train, score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapeley value function\n",
    "The shapeley value will be used in the models to determine the most important features. This is done multiple times so a function will be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(best_pipeline, X):\n",
    "    \"\"\"\n",
    "    Generate SHAP (SHapley Additive exPlanations) values and a summary plot for feature importance.\n",
    "\n",
    "    Parameters:\n",
    "    - best_pipeline (Pipeline): The best-performing pipeline after hyperparameter tuning. It should have an XGBoost regressor named 'xgb_regressor'.\n",
    "    - X (pd.DataFrame): Data to be tested, containing features for which SHAP values will be computed.\n",
    "\n",
    "    Returns:\n",
    "    - importance_df (pd.DataFrame): DataFrame containing feature names and their importance values.\n",
    "    - shap_values (numpy.ndarray): SHAP values for the provided data.\n",
    "\n",
    "    Example:\n",
    "    importance_df, shap_values = feature_importance(best_pipeline=my_best_pipeline, X=my_test_data)\n",
    "    \n",
    "    Note:\n",
    "    The SHAP (SHapley Additive exPlanations) values provide insights into the contribution of each feature to model predictions. The summary plot and importance DataFrame help identify the most influential features.\n",
    "\n",
    "    Dependencies:\n",
    "    - Ensure the 'shap' library is installed. You can install it using 'pip install shap'.\n",
    "\n",
    "    Usage:\n",
    "    - For the best results, pass the best-performing pipeline obtained after hyperparameter tuning. The pipeline should include an XGBoost regressor with the name 'xgb_regressor'.\n",
    "\n",
    "    \"\"\"\n",
    "    # Create a SHAP explainer for the XGBoost model\n",
    "    explainer = shap.Explainer(best_pipeline.named_steps['xgb_regressor'])\n",
    "\n",
    "    # Generate SHAP values\n",
    "    shap_values = explainer.shap_values(X)\n",
    "\n",
    "    # Calculate feature importance using the absolute mean of SHAP values\n",
    "    feature_importance = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "    # Create a DataFrame to associate feature names with their importance values\n",
    "    importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})\n",
    "\n",
    "    # Sort the DataFrame by importance in descending order to find the most important features\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    return importance_df, shap_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "The first model will be used to the determine the risk of someone based on their baseline information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_baseline, baseline_train, baseline_score = train_model(clean_baseline_df, 'Events', ['Record ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feauture importance\n",
    "This information could be used for general information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df_bsaeline, shap_values_baseline  = feature_importance(best_model_baseline, baseline_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SHAP summary plot\n",
    "shap.summary_plot(shap_values_baseline, baseline_train, plot_type=\"bar\", show= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEG model 1\n",
    "The first model will be used to determine the feature importance so the user can then select parameters of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_TEG1, TEG1_train, TEG1_score = train_model(extended_df, 'Events', ['Record ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEG1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df_TEG1, shap_values_TEG1 = feature_importance(best_model_TEG1, TEG1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SHAP summary plot\n",
    "shap.summary_plot(shap_values_TEG1, TEG1_train, plot_type=\"bar\", show= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User interface\n",
    "Here , the user will select the features that they want to test in the next iteration of the model, based on the results from the first model.\n",
    "\n",
    "Streamlit can read strings so for the sake of this notebook streamlit outputs will be printed strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_TEG_df = extended_df.copy()\n",
    "user_TEG_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the most important values from teg. No need for extra created ones\n",
    "if user_extend_data:\n",
    "    columns_to_keep = dict.fromkeys(user_TEG_df.columns.difference(tegValues + new_columns), None)\n",
    "else:\n",
    "    columns_to_keep = dict.fromkeys(user_TEG_df.columns.difference(tegValues), None)\n",
    "\n",
    "# Iterate through prefixes and select the most important column for each\n",
    "for prefix in tegValues:\n",
    "    # Filter the importance_df_TEG1 for the current prefix\n",
    "    prefix_columns = importance_df_TEG1[importance_df_TEG1['Feature'].str.startswith(prefix)]\n",
    "\n",
    "    if not prefix_columns.empty:\n",
    "        # Find the column with the maximum importance for the current prefix\n",
    "        max_importance_row = prefix_columns.loc[prefix_columns['Importance'].idxmax()]\n",
    "\n",
    "        # Check if the maximum importance value is greater than 0\n",
    "        if max_importance_row['Importance'] > 0:\n",
    "            max_importance_column = max_importance_row['Feature']\n",
    "            columns_to_keep[max_importance_column] =max_importance_row['Importance']\n",
    "\n",
    "        else:\n",
    "            columns_to_keep[prefix] = 0\n",
    "\n",
    "columns_to_keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only non repeated values\n",
    "user_TEG_df = user_TEG_df[columns_to_keep.keys()]\n",
    "user_TEG_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload collinear TEG values\n",
    "\n",
    "# Define the filename\n",
    "filename = 'TEG_collinear.json'\n",
    "\n",
    "# Create the full file path by joining the base directory and filename\n",
    "file_path = os.path.join(base_directory, 'data', filename)\n",
    "\n",
    "with open(file_path, 'r') as json_file:\n",
    "    collinearity = json.load(json_file)\n",
    "collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dictionary to hold selection\n",
    "selected_features = {}\n",
    "\n",
    "# Use the dictionary with columns to keep to show user their options\n",
    "for group_name , elements in collinearity.items():\n",
    "\n",
    "    print(group_name) #with st.expander(f\"{group_name}\"):\n",
    "\n",
    "    # Filter keys based on prefixes\n",
    "    filtered_keys = [key for key in columns_to_keep.keys() if any(key.startswith(prefix) for prefix in elements)]\n",
    "\n",
    "    # Create a list of strings by appending keys with values multiplied by 100\n",
    "    radio_labels = [f\"{key} ({round(columns_to_keep[key] * 100, 2)}%)\" for key in filtered_keys]\n",
    "\n",
    "    # Create a radio button to select a feature from the group\n",
    "    print(radio_labels) #selected_feature = st.radio(\"\", radio_labels, key=group_name)\n",
    "    selected_feature = radio_labels[0]\n",
    "    print(type(selected_feature))\n",
    "\n",
    "    # Convert the group list to a tuple and store the selected feature in the dictionary\n",
    "    selected_features[group_name] = selected_feature\n",
    "\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEG model 2\n",
    "After the user selects non-correlated parameters the model will be retrained dropping the values that were not selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all values from selected_features and collinearity\n",
    "selected_features_values = list(selected_features.values())\n",
    "collinearity_values = [item for sublist in collinearity.values() for item in sublist]\n",
    "\n",
    "# Find prefixes to drop\n",
    "prefix_to_keep = [prefix for selection in selected_features_values for prefix in collinearity_values if selection.startswith(prefix)]\n",
    "prefix_to_drop = list(set(collinearity_values) - set(prefix_to_keep))\n",
    "\n",
    "print(prefix_to_keep)\n",
    "prefix_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find list of columns to drop\n",
    "columns_to_drop = [column for column in columns_to_keep.keys() if any(column.startswith(prefix) for prefix in prefix_to_drop)]\n",
    "\n",
    "columns_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_df = user_TEG_df.copy()\n",
    "model2_df.drop(columns=columns_to_drop, inplace=True)\n",
    "model2_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_TEG2, TEG2_train, TEG2_score = train_model(model2_df, 'Events', ['Record ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEG2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df_TEG2, shap_values_TEG2 = feature_importance(best_model_TEG2, TEG2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SHAP summary plot\n",
    "shap.summary_plot(shap_values_TEG2, TEG2_train, plot_type=\"bar\", show= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop before this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script steps.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capstone2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
